{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b763ccb7",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb6e64",
   "metadata": {},
   "source": [
    "architecture screentshot:\n",
    "\n",
    "![](20251120024008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a0e61",
   "metadata": {},
   "source": [
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f86cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "training_data = list(\"\"\"\n",
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset\n",
    "\n",
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d441707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't want to get too deep into tokenization for this notebook so I am just going to instead use all the unique characters\n",
    "# present in the training data as distinct tokens\n",
    "vocabulary_list = list(set(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a57b7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', ',', 'y', 'a', 'f']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_list[:5])\n",
    "print(len(vocabulary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a3b20e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f']\n"
     ]
    }
   ],
   "source": [
    "# let's create training and testing data\n",
    "# training and testing data for next token prediction would look something like\n",
    "\n",
    "# the way the transformer works is that for a single example sentence it trains the model for multiple token prediction\n",
    "print(training_data[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565ee083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here if x is\n",
    "training_data[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0271b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', ' ', 'T', 'r', 'a', 'n', 's', 'f']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then y would be\n",
    "training_data[1:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f34721d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok before we make create training data we need to convert our tokens to a unique index to do that I will do\n",
    "token_to_index = {c:i for i,c in enumerate(vocabulary_list)}\n",
    "index_to_token = {i:c for i,c in enumerate(vocabulary_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we let's convert our training data to a torch tensor\n",
    "import torch\n",
    "\n",
    "training_data_tensor = torch.tensor([token_to_index[c] for c in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37a91543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9, 42, 16, 15, 43,  3,  6, 25,  4, 28])\n",
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f', 'o']\n"
     ]
    }
   ],
   "source": [
    "print(training_data_tensor[:10])\n",
    "print([index_to_token[ix.item()] for ix in training_data_tensor[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create training and testing set\n",
    "block_size = 8\n",
    "x = torch.stack([training_data_tensor[ix:ix+block_size] for ix in range(len(training_data_tensor)-block_size)] )\n",
    "# max ix len(training_data_tensor)-block_size - 1\n",
    "# so ix + block_size = len(training_data_tensor) - 1\n",
    "# so final example won't include last character\n",
    "y = torch.stack([training_data_tensor[ix:ix+block_size]for ix in range(1,len(training_data_tensor)-block_size+1)]) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
