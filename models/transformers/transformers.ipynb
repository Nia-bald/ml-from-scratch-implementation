{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b763ccb7",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb6e64",
   "metadata": {},
   "source": [
    "architecture screentshot:\n",
    "\n",
    "![](20251120024008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a0e61",
   "metadata": {},
   "source": [
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f86cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "training_data = list(\"\"\"\n",
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset\n",
    "\n",
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d441707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't want to get too deep into tokenization for this notebook so I am just going to instead use all the unique characters\n",
    "# present in the training data as distinct tokens\n",
    "vocabulary_list = list(set(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57b7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'I', 'v', 'T', 't']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_list[:5])\n",
    "print(len(vocabulary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a3b20e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f']\n"
     ]
    }
   ],
   "source": [
    "# let's create training and testing data\n",
    "# training and testing data for next token prediction would look something like\n",
    "\n",
    "# the way the transformer works is that for a single example sentence it trains the model for multiple token prediction\n",
    "print(training_data[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565ee083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here if x is\n",
    "training_data[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0271b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', ' ', 'T', 'r', 'a', 'n', 's', 'f']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then y would be\n",
    "training_data[1:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34721d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok before we make create training data we need to convert our tokens to a unique index to do that I will do\n",
    "token_to_index = {c:i for i,c in enumerate(vocabulary_list)}\n",
    "index_to_token = {i:c for i,c in enumerate(vocabulary_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we let's convert our training data to a torch tensor\n",
    "import torch\n",
    "\n",
    "training_data_tensor = torch.tensor([token_to_index[c] for c in training_data], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a91543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14, 28, 42,  3, 16, 36, 35,  0, 43, 24])\n",
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f', 'o']\n"
     ]
    }
   ],
   "source": [
    "print(training_data_tensor[:10])\n",
    "print([index_to_token[ix.item()] for ix in training_data_tensor[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3c920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create training and testing set\n",
    "block_size = 8\n",
    "x = torch.stack([training_data_tensor[ix:ix+block_size] for ix in range(len(training_data_tensor)-block_size)] )\n",
    "# max ix len(training_data_tensor)-block_size - 1\n",
    "# so ix + block_size = len(training_data_tensor) - 1\n",
    "# so final example won't include last character\n",
    "y = torch.stack([training_data_tensor[ix:ix+block_size]for ix in range(1,len(training_data_tensor)-block_size+1)]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc9e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x training data\n",
      "tensor([[14, 28, 42,  3, 16, 36, 35,  0],\n",
      "        [28, 42,  3, 16, 36, 35,  0, 43],\n",
      "        [42,  3, 16, 36, 35,  0, 43, 24],\n",
      "        [ 3, 16, 36, 35,  0, 43, 24, 16],\n",
      "        [16, 36, 35,  0, 43, 24, 16, 39]])\n",
      "y training data\n",
      "tensor([[28, 42,  3, 16, 36, 35,  0, 43],\n",
      "        [42,  3, 16, 36, 35,  0, 43, 24],\n",
      "        [ 3, 16, 36, 35,  0, 43, 24, 16],\n",
      "        [16, 36, 35,  0, 43, 24, 16, 39],\n",
      "        [36, 35,  0, 43, 24, 16, 39, 31]])\n"
     ]
    }
   ],
   "source": [
    "print(\"x training data\")\n",
    "print(x[:5])\n",
    "print(\"y training data\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3e38d",
   "metadata": {},
   "source": [
    "# Embedding Table\n",
    "\n",
    "![](20251121001141.png)\n",
    "\n",
    "\n",
    "This is a look up table between the vocabulary index and n dimensional vector,\n",
    "during the training of transformer model this vectors also gets trained, i.e where these vectors point to gets updated,\n",
    "based on the similarity between these vectors, if let's say I have 2 tokens \"dog\" and \"pooch\", during the start of training process\n",
    "they might point in very different directions, but after the training both would point to pretty much same place\n",
    "\n",
    "### Question?:\n",
    "\n",
    "1. What is so special about the training process that transforms these vectors from pointing in random ass direction, to actually have some meaning\n",
    "    * for now I am gonna assume that the answer is that the transformer architecture expects and assumes these vectors to be what I have described\n",
    "    * and based on this assumption, the subsequent layers performs its operation, so optimizing the loss leads to these embedding vector looking more like actual high dimensional representation of the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "644e2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "EMBEDDING_DIMENSION = 8\n",
    "VOCAB_SIZE = len(vocabulary_list)\n",
    "\n",
    "embeddings_table = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8c861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3082,  0.6139,  0.7662, -0.7353, -0.3764, -0.2977,  1.7620,\n",
      "          -0.3829],\n",
      "         [ 0.4462,  0.3580, -0.8944, -1.2936,  2.0476,  0.2211, -0.7356,\n",
      "           0.4584],\n",
      "         [-0.7255, -1.0177,  1.0224,  2.4789, -0.5182,  0.0262, -0.1825,\n",
      "          -1.0467],\n",
      "         [-0.2908,  0.3825,  1.0557,  1.2988, -0.3164,  0.1577, -0.4467,\n",
      "          -0.3897]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# some experimentation on how embeddings table work,\n",
    "print(embeddings_table(torch.tensor([[0,1,2,3]], dtype=torch.long)))\n",
    "# it goes to each item in tensor and assumes each item is a index converts it to its corresponding embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509c970",
   "metadata": {},
   "source": [
    "I want to do a very simple forward pass so I am gonna create my forward pass batch now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff721a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = x[:5]\n",
    "y_batch = y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfeba10",
   "metadata": {},
   "source": [
    "A question lingers, what does this (shifted right) mean:\n",
    "\n",
    "![](20251121002201.png)\n",
    "\n",
    "this just means that our input is shifted from the target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2beaa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings = embeddings_table(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f7c346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0096,  1.0822,  0.3278,  1.5657,  0.2624,  1.4789, -0.5911,\n",
       "          -1.1504],\n",
       "         [ 0.6911, -0.3817,  1.2578,  0.2299, -1.4089,  1.5734, -0.8249,\n",
       "           0.8280],\n",
       "         [ 2.0004,  1.1353,  0.2975,  0.0307, -0.6738,  1.5019, -0.2457,\n",
       "           1.2371],\n",
       "         [-0.2908,  0.3825,  1.0557,  1.2988, -0.3164,  0.1577, -0.4467,\n",
       "          -0.3897],\n",
       "         [ 0.3388, -0.4310,  1.0913,  1.5326, -2.7795, -0.0863, -0.8432,\n",
       "           0.4986],\n",
       "         [-0.7122, -1.5942, -1.7596,  0.2925,  1.0263,  1.2626,  1.3016,\n",
       "          -0.9153],\n",
       "         [-0.2404, -1.8147,  1.3203,  0.7086, -1.0006,  1.4210, -0.7999,\n",
       "          -0.8074],\n",
       "         [-0.3082,  0.6139,  0.7662, -0.7353, -0.3764, -0.2977,  1.7620,\n",
       "          -0.3829]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just one example\n",
    "x_embeddings[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6ad78",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "![](20251121135418.png)\n",
    "\n",
    "\n",
    "From my past understanding this is sort of values with varies with the position of the token in the sequence to encode the information about the position of the token in the sequence\n",
    "\n",
    "so for each position there will be a vector associated to it, which will get added to the original embedding vector at that position\n",
    "\n",
    "### Questions?:\n",
    "1. Why Add these vectors to the original embedding vector? Can it not be appended or create some other type of encoding create a new channel perhaps like we do for CNNs\n",
    "    - Ans: The Idea behind adding these is how we treat embedding vectors, you can think of embedding vector as the original absolute meaning of a token, now depending on whether it appears at the beggining of a sentence or end of a sentence it's meaning might differ, i.e its embedding vector might change its position, that change is capture by the addition of this positional embeddding vector\n",
    "2. Why do these needs to be a vector all together can these not be like a single number which gets added?\n",
    "    - Ans: well a vector is a more generalized version of a single number, if single number is the right approach then expectation is that the network would train the embedings to become a single number\n",
    "\n",
    "## Sinusoidal Encoding\n",
    "\n",
    "![](20251121140214.png)\n",
    "\n",
    "here d_model is the dimension of the embedding\n",
    "\n",
    "In the original Paper they used a fix positional sinusoidal encoding, they mentioned the performance for both learned and not learned were identical, they wanted to experiment with sinusoidal encoding, because they wanted to test the model beyond the trained context length\n",
    "\n",
    "# Question?:\n",
    "1. But why sinusoidal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810c2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embedding_table = nn.Embedding(block_size, EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6e839a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos_embeddings = positional_embedding_table(torch.arange(x_embeddings.shape[1])) # C, E\n",
    "\n",
    "x_embeddings # B, C, E\n",
    "\n",
    "x_embeddings_total = x_embeddings + x_pos_embeddings # B, C, E + C, E -> pytorch checks the shape starting from right and if there is an extra dimension it creates a new dimention and copuies the same thing over, like C, E -> (1, C, E) -> (B, C, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f61c12",
   "metadata": {},
   "source": [
    "# Self Attention Layer\n",
    "\n",
    "![](20251123232032.png)\n",
    "\n",
    "I will start of by explaining what this layer does in a high level, then I will dig deep into how it does this, initially I will go over a single head self attention, \n",
    "then understand myself and explain why multi head self attention\n",
    "\n",
    "this is the 3b1b interpretation of this layer on a high level, which I found to be the most elegant\n",
    "\n",
    "## The Explanation\n",
    "This layer as a whole tells us how should the original embedding vector be modified, so that it's meaning is enriched with the context of the surrounding tokens, for example take the sentence:\n",
    "\n",
    "\" That blue aeroplane is very dangerous \"\n",
    "\n",
    "in this example initially \"aeroplane\"'s embedding vector would straight up point to the absolute aeroplane,\n",
    "then attention layer outputs a result, that result when added to the original embedding vector, nudges the aeroplane's vector in a direction closer to blue and dangerous\n",
    "\n",
    "that is on high level what this layer does, now going into the detail let's start by the equation\n",
    "\n",
    "![](20251123233808.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f5c08",
   "metadata": {},
   "source": [
    "the above represent the equation describing the self attention mechanism, for the purpose of this excercise we will focus on masked self attention\n",
    "\n",
    "here Q, K, V are all matrices\n",
    "\n",
    "Q being the query matrix, K Key matrix and V value matrix\n",
    "\n",
    "let me do one thing and form this forumla in our on going example and then explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4dff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = d_q = d_v = 10\n",
    "\n",
    "W_q = nn.Linear(EMBEDDING_DIMENSION, d_q, bias = False)\n",
    "W_k = nn.Linear(EMBEDDING_DIMENSION, d_k, bias = False)\n",
    "W_v = nn.Linear(EMBEDDING_DIMENSION, d_v, bias = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9a8f7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1780a725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embeddings_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "039e3642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7523, -1.1425, -0.1237, -0.4978, -1.3365, -1.1024,  0.6787, -0.0977,\n",
       "         0.8006, -0.3372], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.weight@x_embeddings_total[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e849c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.5233e-01, -1.1425e+00, -1.2365e-01, -4.9778e-01, -1.3365e+00,\n",
       "          -1.1024e+00,  6.7866e-01, -9.7705e-02,  8.0059e-01, -3.3718e-01],\n",
       "         [ 1.1098e-01, -4.9259e-01,  1.3709e+00,  2.9401e-01, -1.6201e+00,\n",
       "          -4.2740e-01,  1.1961e+00,  1.3074e-01, -3.8340e-02, -4.5341e-01],\n",
       "         [ 3.7432e-01, -8.8783e-02,  1.3748e+00,  3.0277e-01, -1.8223e+00,\n",
       "           4.8749e-01,  1.0525e+00,  3.2254e-01, -7.8679e-01,  5.9447e-01],\n",
       "         [-5.5625e-01, -9.4867e-02, -7.4363e-01,  2.6241e-01,  5.8117e-01,\n",
       "           1.2421e-01, -4.3674e-01, -5.6245e-01,  6.7975e-01, -6.9114e-01],\n",
       "         [-5.6364e-01, -1.2077e+00, -6.3766e-01,  6.0641e-01,  6.3862e-01,\n",
       "          -9.8387e-01,  7.4406e-01,  3.5998e-02,  6.0114e-01, -1.5101e+00],\n",
       "         [ 9.8003e-01, -1.3104e-01,  9.7843e-01,  5.3600e-01,  6.3538e-01,\n",
       "          -1.9702e-01,  1.0436e+00,  1.2430e+00, -5.9683e-01, -3.5266e-01],\n",
       "         [-1.1023e+00, -1.2548e+00, -6.0360e-01,  1.6455e-01, -6.3066e-01,\n",
       "          -5.7828e-01, -7.9755e-03, -5.5825e-01,  9.1867e-01, -1.0980e+00],\n",
       "         [ 5.2344e-02,  1.8281e-01, -2.8731e-01, -3.7783e-01,  2.2183e-01,\n",
       "          -6.0821e-01, -9.4986e-01, -3.3315e-01,  4.3243e-01, -2.1380e-01]],\n",
       "\n",
       "        [[-1.0350e+00, -6.7913e-01, -6.0657e-01,  6.5492e-01, -9.4018e-01,\n",
       "          -1.2915e+00,  1.4033e+00, -1.4468e+00,  7.3514e-01, -1.2692e+00],\n",
       "         [ 1.0148e+00,  4.1104e-01,  2.3557e+00, -1.6575e-01, -1.9809e+00,\n",
       "          -4.3212e-01,  9.6016e-01,  3.8450e-01, -7.8796e-01,  4.4708e-01],\n",
       "         [-6.1428e-01, -1.4101e+00,  2.0917e-02, -2.2085e-01, -1.1492e+00,\n",
       "           4.2587e-01,  2.3443e-01,  7.8410e-01,  1.6688e-01,  2.4751e-01],\n",
       "         [-6.9556e-01,  1.8973e-01, -1.2226e+00,  9.7353e-01,  9.9502e-01,\n",
       "          -6.3988e-01,  4.2776e-01, -1.5013e+00,  8.6687e-01, -1.5242e+00],\n",
       "         [ 6.6757e-01, -4.4063e-01,  4.8566e-01,  2.3389e-01,  1.0414e+00,\n",
       "           9.0717e-01, -3.7261e-02,  1.8234e+00, -3.9894e-02, -8.0750e-02],\n",
       "         [-5.0263e-01, -1.6747e+00,  1.5281e-02,  1.0833e+00, -1.8975e-01,\n",
       "          -1.0644e+00,  1.8125e+00,  2.4329e-01,  1.9128e-01, -1.6939e+00],\n",
       "         [ 2.3435e-01,  7.2675e-01,  3.1406e-01, -6.2592e-01, -3.5972e-01,\n",
       "          -5.9582e-01, -1.6956e+00, -8.5572e-01,  1.8463e-01, -2.0672e-02],\n",
       "         [-2.5420e-01, -7.1129e-02,  4.1173e-01,  3.2482e-01, -9.3732e-01,\n",
       "          -5.0822e-02,  6.7812e-01, -5.1865e-01,  1.6655e-02, -2.5692e-01]],\n",
       "\n",
       "        [[-1.3116e-01,  2.2449e-01,  3.7821e-01,  1.9516e-01, -1.3010e+00,\n",
       "          -1.2962e+00,  1.1674e+00, -1.1930e+00, -1.4481e-02, -3.6868e-01],\n",
       "         [ 2.6218e-02, -9.1023e-01,  1.0018e+00, -6.8937e-01, -1.3079e+00,\n",
       "          -4.9375e-01,  1.4209e-01,  8.4606e-01,  1.6571e-01,  1.0013e-01],\n",
       "         [-7.5359e-01, -1.1255e+00, -4.5810e-01,  4.9028e-01, -7.3538e-01,\n",
       "          -3.3822e-01,  1.0989e+00, -1.5473e-01,  3.5400e-01, -5.8550e-01],\n",
       "         [ 5.3564e-01,  9.5681e-01, -9.9326e-02,  6.0101e-01,  1.3978e+00,\n",
       "           1.2512e+00, -3.5356e-01,  2.8611e-01,  2.2584e-01, -9.4777e-02],\n",
       "         [-8.1509e-01, -1.9843e+00, -4.7749e-01,  7.8114e-01,  2.1626e-01,\n",
       "           3.9749e-02,  7.3161e-01,  8.2362e-01,  7.4822e-01, -1.4219e+00],\n",
       "         [ 8.3406e-01,  3.0687e-01,  9.3293e-01,  2.9279e-01,  8.1189e-02,\n",
       "          -1.0820e+00,  1.2492e-01, -5.4174e-02, -5.4275e-01, -6.1657e-01],\n",
       "         [-7.2199e-02,  4.7281e-01,  1.0131e+00,  7.6726e-02, -1.5189e+00,\n",
       "          -3.8431e-02, -6.7583e-02, -1.0412e+00, -2.3114e-01, -6.3793e-02],\n",
       "         [-6.3424e-01, -5.5174e-01, -1.1498e+00, -7.8297e-01, -4.2673e-01,\n",
       "          -1.3251e+00, -6.2151e-01, -5.9781e-01,  8.5225e-01, -4.8308e-02]],\n",
       "\n",
       "        [[-1.1198e+00, -1.0968e+00, -9.7566e-01, -3.2846e-01, -6.2797e-01,\n",
       "          -1.3578e+00,  3.4932e-01, -7.3144e-01,  9.3919e-01, -7.1563e-01],\n",
       "         [-1.1310e-01, -6.2564e-01,  5.2280e-01,  2.1751e-02, -8.9401e-01,\n",
       "          -1.2578e+00,  1.0066e+00, -9.2766e-02,  3.5283e-01, -7.3289e-01],\n",
       "         [ 4.7761e-01, -3.5838e-01,  6.6522e-01,  1.1776e-01, -3.3261e-01,\n",
       "           1.5528e+00,  3.1761e-01,  1.6327e+00, -2.8703e-01,  8.4388e-01],\n",
       "         [-9.4702e-01, -5.8683e-01, -1.0625e+00,  1.1483e+00,  5.7266e-01,\n",
       "           3.8374e-01,  4.1531e-01, -7.1364e-01,  1.0139e+00, -1.4360e+00],\n",
       "         [ 5.2159e-01, -2.7250e-03,  4.4017e-01, -9.3271e-03,  4.8720e-01,\n",
       "           2.2215e-02, -9.5598e-01,  5.2616e-01,  1.4181e-02, -3.4466e-01],\n",
       "         [ 5.2751e-01,  5.2922e-02,  1.6320e+00,  9.9544e-01, -1.0780e+00,\n",
       "          -5.2459e-01,  1.7529e+00, -2.3968e-01, -9.5853e-01, -6.5969e-01],\n",
       "         [-4.5224e-01, -7.8044e-03, -5.4841e-01, -1.0311e+00, -1.0083e+00,\n",
       "          -1.3127e+00, -1.3672e+00, -1.1204e+00,  6.0445e-01,  1.4482e-01],\n",
       "         [-1.0329e+00, -1.0222e+00, -1.3651e+00,  2.3790e-01,  3.7325e-01,\n",
       "          -1.6143e+00,  7.5018e-01, -8.2331e-01,  1.0194e+00, -1.3793e+00]],\n",
       "\n",
       "        [[-1.2591e+00, -8.1218e-01, -1.4547e+00,  3.8266e-01, -2.1412e-01,\n",
       "          -2.1219e+00,  1.2138e+00, -1.6703e+00,  1.1263e+00, -1.5486e+00],\n",
       "         [ 1.1181e+00,  1.4144e-01,  1.6461e+00, -3.5077e-01, -4.9124e-01,\n",
       "           6.3321e-01,  2.2527e-01,  1.6946e+00, -2.8820e-01,  6.9649e-01],\n",
       "         [-1.0050e+00, -1.9020e+00, -2.9792e-01,  6.6501e-01, -1.1577e+00,\n",
       "           6.8540e-01,  1.0865e+00,  6.3290e-01,  5.0108e-01, -4.9731e-01],\n",
       "         [ 3.8966e-01,  1.3947e+00, -1.4482e-01,  3.5780e-01,  8.4360e-01,\n",
       "           3.6621e-01, -1.2723e+00, -1.0111e+00,  2.7991e-01, -3.5868e-01],\n",
       "         [ 2.1505e-01, -2.5667e-01,  1.1392e+00,  6.9332e-01, -6.7196e-01,\n",
       "           5.7960e-01,  6.7200e-01,  3.4066e-01, -4.0159e-01, -3.8778e-01],\n",
       "         [ 1.4747e-01, -4.2769e-01,  7.0463e-02, -1.1236e-01, -5.6738e-01,\n",
       "          -1.7989e+00,  4.5327e-01, -3.1884e-01, -1.2294e-01, -4.5107e-01],\n",
       "         [-8.5088e-01, -4.7823e-01, -7.6377e-01, -1.0192e-02, -2.0830e-01,\n",
       "          -1.6019e+00,  4.4748e-03, -1.3459e+00,  7.7159e-01, -1.1861e+00],\n",
       "         [ 2.4769e-01,  4.9181e-01,  1.2400e-01,  6.4313e-01,  1.1857e-04,\n",
       "           4.7500e-02,  1.5253e-02, -5.4616e-01,  9.0750e-02, -4.6645e-01]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q(x_embeddings_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5a1c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the existing vector through a trainable linear transformation\n",
    "\n",
    "Q, K, V = W_q(x_embeddings_total), W_k(x_embeddings_total), W_v(x_embeddings_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8f7801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 10]) torch.Size([5, 8, 10]) torch.Size([5, 8, 10])\n"
     ]
    }
   ],
   "source": [
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c16f6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_matrix = Q@K.transpose(1, 2) # transpose the 1st and the 2nd dimension not the  this is equivalent to Q[i]@K[i].transpose where i is each element of the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35fe6ae",
   "metadata": {},
   "source": [
    "Let me try and explain what just happened above, let's take example of a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a6b1b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7523, -1.1425, -0.1237, -0.4978, -1.3365, -1.1024,  0.6787, -0.0977,\n",
      "          0.8006, -0.3372],\n",
      "        [ 0.1110, -0.4926,  1.3709,  0.2940, -1.6201, -0.4274,  1.1961,  0.1307,\n",
      "         -0.0383, -0.4534],\n",
      "        [ 0.3743, -0.0888,  1.3748,  0.3028, -1.8223,  0.4875,  1.0525,  0.3225,\n",
      "         -0.7868,  0.5945]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.8104, -1.1986, -1.3065, -1.1477, -0.3196,  1.1467, -0.0293,  0.5532,\n",
      "         -0.0752, -1.3992],\n",
      "        [ 0.5583,  0.1986,  0.1202, -1.3505, -0.7072,  1.2815,  0.2118, -0.8598,\n",
      "          0.0324, -0.5253],\n",
      "        [-0.0195,  1.0557,  1.5437,  0.2219, -0.8512,  1.2104,  0.3387, -1.9492,\n",
      "          0.4774, -0.3484]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(Q[0][:3])\n",
    "print(K[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730f50d",
   "metadata": {},
   "source": [
    "well the traditional explanation is:\n",
    "\n",
    "when the static embedding for a word is passed through these layers it extract specific feature pertaining to corresponding transformation\n",
    "\n",
    "let's take an example: \"The bank of the River\"\n",
    "\n",
    "Query -> transform the static embedding of bank to something like \"I am a Noun needing a definition, need to know what am I a river bank, a financial bank, etc\"\n",
    "\n",
    "Key -> transforms the static embedding of River to something like \"I am a nature related word, related to river\"\n",
    "\n",
    "Value -> transforms the embedding to actual meaning info, as their it might not be the first attention layer, if it is a second layer, it won't be the absolute meaning\n",
    "\n",
    "but this never really sat with me completely, I was not able to understand this fully and it seemed pretty handwavy as to explaining what these layers really do, maybe I understand for Value vector but not for Query and Key vector\n",
    "\n",
    "need to think and understand this properly and clearly once and for all, \n",
    "think, what would happen if no linear layer was present\n",
    "\n",
    "\n",
    "ok let's say no linear layer was present, than the Q*K.T would just give me the cosine similarity between static embeddings of tokens\n",
    "\n",
    "let's say the attention matrix is\n",
    "\n",
    "A\n",
    "\n",
    "here A[i][j] would be the cosine similarity between the ith token and jth token  (here when I say token I mean the embedding of token)\n",
    "\n",
    "now let's see what would A*V would do, here I am assuming V to is just the static emebdding\n",
    "\n",
    "\n",
    "A_softmax -> is softmax function applied on A across each column\n",
    "\n",
    "```\n",
    "\n",
    "V`[i] = V[0]*A_softmax[i][0] + V[1]*A_softmax[i][1] + V[2]*A_softmax[i][2] + ...\n",
    "\n",
    "```\n",
    "\n",
    "here `V[i]` represents the static embedding for the ith word, so `V[i]` is a vector definitely, and ``V`[i]`` is the final vector after attention matrix multiplication\n",
    "\n",
    "``V`[i]`` we can think of it as change or how the original word gets modified\n",
    "\n",
    "so when taking the weighted sum it tells us, in direction of which static embedding should the input embedding move to, so that it represents that single vector would represent the meaning of the entire sentence, **now I see the poblem with just using static embedding, the problem is if we just use static embedding to compute cosine similarity it would nudge the input emebedding to this layer in the direction which is most similar to the word which we see in the sentence, which we don't always want**\n",
    "\n",
    "side note: one more thing I learned is that these query and key vector live in a space which is smaller than the static embedding dimension, so it would help computationaly too\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3dceac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ff050f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now each element of this of shape context length x context length\n",
    "attention_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791be25d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
