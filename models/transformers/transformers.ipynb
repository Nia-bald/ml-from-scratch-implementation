{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b763ccb7",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb6e64",
   "metadata": {},
   "source": [
    "architecture screentshot:\n",
    "\n",
    "![](20251120024008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a0e61",
   "metadata": {},
   "source": [
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f86cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "training_data = list(\"\"\"\n",
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset\n",
    "\n",
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d441707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't want to get too deep into tokenization for this notebook so I am just going to instead use all the unique characters\n",
    "# present in the training data as distinct tokens\n",
    "vocabulary_list = list(set(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57b7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'k', 'f', 'm', 'x']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_list[:5])\n",
    "print(len(vocabulary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a3b20e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f']\n"
     ]
    }
   ],
   "source": [
    "# let's create training and testing data\n",
    "# training and testing data for next token prediction would look something like\n",
    "\n",
    "# the way the transformer works is that for a single example sentence it trains the model for multiple token prediction\n",
    "print(training_data[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565ee083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here if x is\n",
    "training_data[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0271b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', ' ', 'T', 'r', 'a', 'n', 's', 'f']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then y would be\n",
    "training_data[1:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34721d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok before we make create training data we need to convert our tokens to a unique index to do that I will do\n",
    "token_to_index = {c:i for i,c in enumerate(vocabulary_list)}\n",
    "index_to_token = {i:c for i,c in enumerate(vocabulary_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we let's convert our training data to a torch tensor\n",
    "import torch\n",
    "\n",
    "training_data_tensor = torch.tensor([token_to_index[c] for c in training_data], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a91543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21, 11, 23, 22, 24, 42, 30, 17,  2, 12])\n",
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f', 'o']\n"
     ]
    }
   ],
   "source": [
    "print(training_data_tensor[:10])\n",
    "print([index_to_token[ix.item()] for ix in training_data_tensor[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3c920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create training and testing set\n",
    "block_size = 8\n",
    "x = torch.stack([training_data_tensor[ix:ix+block_size] for ix in range(len(training_data_tensor)-block_size)] )\n",
    "# max ix len(training_data_tensor)-block_size - 1\n",
    "# so ix + block_size = len(training_data_tensor) - 1\n",
    "# so final example won't include last character\n",
    "y = torch.stack([training_data_tensor[ix:ix+block_size]for ix in range(1,len(training_data_tensor)-block_size+1)]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc9e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x training data\n",
      "tensor([[21, 11, 23, 22, 24, 42, 30, 17],\n",
      "        [11, 23, 22, 24, 42, 30, 17,  2],\n",
      "        [23, 22, 24, 42, 30, 17,  2, 12],\n",
      "        [22, 24, 42, 30, 17,  2, 12, 24],\n",
      "        [24, 42, 30, 17,  2, 12, 24,  3]])\n",
      "y training data\n",
      "tensor([[11, 23, 22, 24, 42, 30, 17,  2],\n",
      "        [23, 22, 24, 42, 30, 17,  2, 12],\n",
      "        [22, 24, 42, 30, 17,  2, 12, 24],\n",
      "        [24, 42, 30, 17,  2, 12, 24,  3],\n",
      "        [42, 30, 17,  2, 12, 24,  3, 18]])\n"
     ]
    }
   ],
   "source": [
    "print(\"x training data\")\n",
    "print(x[:5])\n",
    "print(\"y training data\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3e38d",
   "metadata": {},
   "source": [
    "# Embedding Table\n",
    "\n",
    "![](20251121001141.png)\n",
    "\n",
    "\n",
    "This is a look up table between the vocabulary index and n dimensional vector,\n",
    "during the training of transformer model this vectors also gets trained, i.e where these vectors point to gets updated,\n",
    "based on the similarity between these vectors, if let's say I have 2 tokens \"dog\" and \"pooch\", during the start of training process\n",
    "they might point in very different directions, but after the training both would point to pretty much same place\n",
    "\n",
    "### Question?:\n",
    "\n",
    "1. What is so special about the training process that transforms these vectors from pointing in random ass direction, to actually have some meaning\n",
    "    * for now I am gonna assume that the answer is that the transformer architecture expects and assumes these vectors to be what I have described\n",
    "    * and based on this assumption, the subsequent layers performs its operation, so optimizing the loss leads to these embedding vector looking more like actual high dimensional representation of the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "644e2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "EMBEDDING_DIMENSION = 8\n",
    "VOCAB_SIZE = len(vocabulary_list)\n",
    "\n",
    "embeddings_table = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8c861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5037, -0.4606, -1.0284,  1.4244, -0.6913, -2.1196,  0.4278,\n",
      "          -1.0677],\n",
      "         [-0.2712, -0.2181,  2.1937,  0.5109,  0.0822,  0.1224, -0.7383,\n",
      "          -2.5617],\n",
      "         [ 0.7080,  0.7884, -0.1744, -0.4616,  1.0200,  1.2690,  0.5349,\n",
      "           1.8285],\n",
      "         [-0.5165,  1.6759,  1.0185, -1.1399, -0.1933,  0.6807, -0.3136,\n",
      "           1.7759]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# some experimentation on how embeddings table work,\n",
    "print(embeddings_table(torch.tensor([[0,1,2,3]], dtype=torch.long)))\n",
    "# it goes to each item in tensor and assumes each item is a index converts it to its corresponding embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509c970",
   "metadata": {},
   "source": [
    "I want to do a very simple forward pass so I am gonna create my forward pass batch now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff721a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = x[:5]\n",
    "y_batch = y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfeba10",
   "metadata": {},
   "source": [
    "A question lingers, what does this (shifted right) mean:\n",
    "\n",
    "![](20251121002201.png)\n",
    "\n",
    "this just means that our input is shifted from the target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2beaa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings = embeddings_table(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f7c346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3670, -0.5352, -0.5615,  1.2713,  0.7045, -2.3189,  1.6717,\n",
       "          -0.8195],\n",
       "         [ 0.0431,  0.8193, -0.6997,  0.1229,  1.8298, -0.6548,  1.1824,\n",
       "           0.0806],\n",
       "         [ 1.2184, -0.8640,  0.3499,  1.4709,  0.0571,  0.6448,  0.6276,\n",
       "           0.3043],\n",
       "         [ 0.8728, -0.2741,  0.2480,  0.7597, -1.7264,  0.1798, -0.4858,\n",
       "           0.6839],\n",
       "         [-1.5515, -0.4268,  0.4334,  0.2148, -1.8364,  0.0896,  1.1805,\n",
       "           0.2158],\n",
       "         [-0.8852, -0.6178, -0.5703, -0.4105,  0.3587,  0.3294,  0.0929,\n",
       "          -0.2885],\n",
       "         [-0.3182, -0.4844, -0.9233,  0.8547, -1.1227, -0.1346, -0.8138,\n",
       "          -1.2769],\n",
       "         [-0.1046, -1.3187, -0.6789, -0.6506,  0.0127, -0.3080, -0.6245,\n",
       "          -0.0633]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just one example\n",
    "x_embeddings[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6ad78",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "![](20251121135418.png)\n",
    "\n",
    "\n",
    "From my past understanding this is sort of values with varies with the position of the token in the sequence to encode the information about the position of the token in the sequence\n",
    "\n",
    "so for each position there will be a vector associated to it, which will get added to the original embedding vector at that position\n",
    "\n",
    "### Questions?:\n",
    "1. Why Add these vectors to the original embedding vector? Can it not be appended or create some other type of encoding create a new channel perhaps like we do for CNNs\n",
    "    - Ans: The Idea behind adding these is how we treat embedding vectors, you can think of embedding vector as the original absolute meaning of a token, now depending on whether it appears at the beggining of a sentence or end of a sentence it's meaning might differ, i.e its embedding vector might change its position, that change is capture by the addition of this positional embeddding vector\n",
    "2. Why do these needs to be a vector all together can these not be like a single number which gets added?\n",
    "    - Ans: well a vector is a more generalized version of a single number, if single number is the right approach then expectation is that the network would train the embedings to become a single number\n",
    "\n",
    "## Sinusoidal Encoding\n",
    "\n",
    "![](20251121140214.png)\n",
    "\n",
    "here d_model is the dimension of the embedding\n",
    "\n",
    "In the original Paper they used a fix positional sinusoidal encoding, they mentioned the performance for both learned and not learned were identical, they wanted to experiment with sinusoidal encoding, because they wanted to test the model beyond the trained context length\n",
    "\n",
    "# Question?:\n",
    "1. But why sinusoidal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810c2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embedding_table = nn.Embedding(block_size, EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6e839a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos_embeddings = positional_embedding_table(torch.arange(x_embeddings.shape[1])) # C, E\n",
    "\n",
    "x_embeddings # B, C, E\n",
    "\n",
    "x_embeddings_total = x_embeddings + x_pos_embeddings # B, C, E + C, E -> pytorch checks the shape starting from right and if there is an extra dimension it creates a new dimention and copuies the same thing over, like C, E -> (1, C, E) -> (B, C, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f61c12",
   "metadata": {},
   "source": [
    "# Self Attention Layer\n",
    "\n",
    "![](20251123232032.png)\n",
    "\n",
    "I will start of by explaining what this layer does in a high level, then I will dig deep into how it does this, initially I will go over a single head self attention, \n",
    "then understand myself and explain why multi head self attention\n",
    "\n",
    "this is the 3b1b interpretation of this layer on a high level, which I found to be the most elegant\n",
    "\n",
    "## The Explanation\n",
    "This layer as a whole tells us how should the original embedding vector be modified, so that it's meaning is enriched with the context of the surrounding tokens, for example take the sentence:\n",
    "\n",
    "\" That blue aeroplane is very dangerous \"\n",
    "\n",
    "in this example initially \"aeroplane\"'s embedding vector would straight up point to the absolute aeroplane,\n",
    "then attention layer outputs a result, that result when added to the original embedding vector, nudges the aeroplane's vector in a direction closer to blue and dangerous\n",
    "\n",
    "that is on high level what this layer does, now going into the detail let's start by the equation\n",
    "\n",
    "![](20251123233808.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f5c08",
   "metadata": {},
   "source": [
    "the above represent the equation describing the self attention mechanism, for the purpose of this excercise we will focus on masked self attention\n",
    "\n",
    "here Q, K, V are all matrices\n",
    "\n",
    "Q being the query matrix, K Key matrix and V value matrix\n",
    "\n",
    "let me do one thing and form this forumla in our on going example and then explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4dff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = d_q = 4\n",
    "\n",
    "W_q = nn.Linear(EMBEDDING_DIMENSION, d_q, bias = False)\n",
    "W_k = nn.Linear(EMBEDDING_DIMENSION, d_k, bias = False)\n",
    "W_v = nn.Linear(EMBEDDING_DIMENSION, EMBEDDING_DIMENSION, bias = False) # for single head attention this needs to be same as embedding dimension cause the resulting vector of attention layer get added to input embedding vector so both need to have same dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9a8f7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1780a725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embeddings_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "039e3642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2962,  0.4433, -0.4033,  0.0918], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.weight@x_embeddings_total[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e849c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9622e-01,  4.4334e-01, -4.0333e-01,  9.1819e-02],\n",
       "         [ 2.0940e-01, -1.9907e+00, -1.2677e+00, -2.5409e+00],\n",
       "         [-4.4036e-01, -1.0597e+00, -9.9490e-01, -4.0069e-01],\n",
       "         [ 3.5352e-02,  6.0534e-01,  7.5681e-02,  1.6958e+00],\n",
       "         [ 2.1072e-01,  5.2485e-01,  5.1694e-01,  6.2034e-01],\n",
       "         [-5.7203e-01,  8.1172e-01, -2.6101e-01, -6.2432e-01],\n",
       "         [ 4.2714e-01,  1.7028e-01,  3.5651e-01,  6.0556e-01],\n",
       "         [ 4.0804e-01, -3.5315e-01, -3.4155e-01,  6.7602e-01]],\n",
       "\n",
       "        [[ 1.9107e-01,  4.4825e-03, -1.1271e+00, -8.4113e-01],\n",
       "         [ 5.5712e-01, -2.5121e+00, -1.2270e+00, -1.6842e+00],\n",
       "         [-7.1933e-01,  2.8078e-01, -6.8147e-01,  4.5110e-01],\n",
       "         [-8.0519e-01,  9.6637e-01,  8.9220e-01,  1.6046e+00],\n",
       "         [ 1.3123e+00,  2.8796e-01,  6.3086e-04, -3.2865e-01],\n",
       "         [-5.9724e-01,  1.2756e+00, -2.9307e-01, -3.9490e-01],\n",
       "         [ 1.0126e+00,  1.7249e-01,  5.3831e-01,  1.0307e+00],\n",
       "         [-3.2659e-01, -2.1117e+00, -1.6715e+00, -3.2419e-01]],\n",
       "\n",
       "        [[ 5.3879e-01, -5.1690e-01, -1.0864e+00,  1.5568e-02],\n",
       "         [ 2.7815e-01, -1.1716e+00, -9.1360e-01, -8.3243e-01],\n",
       "         [-1.5599e+00,  6.4180e-01,  1.3505e-01,  3.5984e-01],\n",
       "         [ 2.9642e-01,  7.2948e-01,  3.7589e-01,  6.5558e-01],\n",
       "         [ 1.2871e+00,  7.5185e-01, -3.1434e-02, -9.9241e-02],\n",
       "         [-1.1737e-02,  1.2778e+00, -1.1127e-01,  3.0278e-02],\n",
       "         [ 2.7801e-01, -1.5861e+00, -7.9165e-01,  3.0526e-02],\n",
       "         [ 4.8620e-02, -2.0685e+00, -4.7481e-01,  2.2808e-01]],\n",
       "\n",
       "        [[ 2.5982e-01,  8.2359e-01, -7.7299e-01,  8.6736e-01],\n",
       "         [-5.6239e-01, -8.1054e-01, -9.7087e-02, -9.2369e-01],\n",
       "         [-4.5826e-01,  4.0492e-01, -3.8126e-01, -5.8916e-01],\n",
       "         [ 2.7120e-01,  1.1934e+00,  3.4382e-01,  8.8499e-01],\n",
       "         [ 1.8726e+00,  7.5405e-01,  1.5036e-01,  3.2594e-01],\n",
       "         [-7.4637e-01, -4.8077e-01, -1.4412e+00, -9.6994e-01],\n",
       "         [ 6.5322e-01, -1.5429e+00,  4.0505e-01,  5.8280e-01],\n",
       "         [-1.2539e+00, -5.8236e-01,  2.5031e-02,  9.7042e-01]],\n",
       "\n",
       "        [[-5.8072e-01,  1.1846e+00,  4.3527e-02,  7.7609e-01],\n",
       "         [ 5.3922e-01, -1.0474e+00, -6.1340e-01, -1.8727e+00],\n",
       "         [-4.8348e-01,  8.6880e-01, -4.1333e-01, -3.5975e-01],\n",
       "         [ 8.5671e-01,  1.1956e+00,  5.2562e-01,  1.3102e+00],\n",
       "         [ 1.1380e+00, -1.0045e+00, -1.1796e+00, -6.7427e-01],\n",
       "         [-3.7116e-01, -4.3753e-01, -2.4453e-01, -4.1766e-01],\n",
       "         [-6.4925e-01, -5.6720e-02,  9.0489e-01,  1.3251e+00],\n",
       "         [-1.2850e+00, -1.0250e+00, -8.0853e-01,  5.5307e-01]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q(x_embeddings_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5a1c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the existing vector through a trainable linear transformation\n",
    "\n",
    "Q, K, V = W_q(x_embeddings_total), W_k(x_embeddings_total), W_v(x_embeddings_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8f7801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 4]) torch.Size([5, 8, 4]) torch.Size([5, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c16f6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_matrix = Q@K.transpose(1, 2) # transpose the 1st and the 2nd dimension not the  this is equivalent to Q[i]@K[i].transpose where i is each element of the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35fe6ae",
   "metadata": {},
   "source": [
    "Let me try and explain what just happened above, let's take example of a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a6b1b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2962,  0.4433, -0.4033,  0.0918],\n",
      "        [ 0.2094, -1.9907, -1.2677, -2.5409],\n",
      "        [-0.4404, -1.0597, -0.9949, -0.4007]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4785, -1.8924,  1.0503,  0.9186],\n",
      "        [ 1.7903,  0.0951,  0.1838, -0.3514],\n",
      "        [-0.3516,  0.0406,  0.7802,  1.3018]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(Q[0][:3])\n",
    "print(K[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730f50d",
   "metadata": {},
   "source": [
    "\n",
    "well the traditional explanation is:\n",
    "\n",
    "when the static embedding for a word is passed through these layers it extract specific feature pertaining to corresponding transformation\n",
    "\n",
    "let's take an example: \"The bank of the River\"\n",
    "\n",
    "Query -> transform the static embedding of bank to something like \"I am a Noun needing a definition, need to know what am I a river bank, a financial bank, etc\"\n",
    "\n",
    "Key -> transforms the static embedding of River to something like \"I am a nature related word, related to river\"\n",
    "\n",
    "Value -> transforms the embedding to actual meaning info, as their it might not be the first attention layer, if it is a second layer, it won't be the absolute meaning\n",
    "\n",
    "but this never really sat with me completely, I was not able to understand this fully and it seemed pretty handwavy as to explaining what these layers really do, maybe I understand for Value vector but not for Query and Key vector\n",
    "\n",
    "need to think and understand this properly and clearly once and for all, \n",
    "think, what would happen if no linear layer was present\n",
    "\n",
    "\n",
    "ok let's say no linear layer was present, than the Q*K.T would just give me the cosine similarity between static embeddings of tokens\n",
    "\n",
    "let's say the attention matrix is\n",
    "\n",
    "`A`\n",
    "\n",
    "here `A[i][j]` would be the cosine similarity between the ith token and jth token  (here when I say token I mean the embedding of token)\n",
    "\n",
    "now let's see what would `A@V` do, here I am assuming `V` to is just the static emebdding\n",
    "\n",
    "\n",
    "`A_softmax` -> is softmax function applied on `A` across each column (why column? cause weights are distributed across column see below)\n",
    "\n",
    "```\n",
    "\n",
    "V`[i] = V[0]*A_softmax[i][0] + V[1]*A_softmax[i][1] + V[2]*A_softmax[i][2] + ...\n",
    "\n",
    "```\n",
    "\n",
    "here `V[i]` represents the static embedding for the ith word, so `V[i]` is a vector, and ``V`[i]`` is the final vector after attention matrix multiplication\n",
    "\n",
    "``V`[i]`` we can think of it as change or how the original word gets modified\n",
    "\n",
    "so when taking the weighted sum it tells us, in direction of which static embedding should the input embedding move to the most and least (depending on the weight), so that it represents that modified vector would represent the meaning of the entire sentence, **now I see the poblem with just using static embedding, the problem is if we just use static embedding to compute cosine similarity it would nudge the input emebedding to this layer in the direction which is most similar to the word which we see in the sentence, which we don't always want**\n",
    "\n",
    "side note: one more thing I learned is that these query and key vector live in a space which is smaller than the static embedding dimension, so it would help computationaly too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ff050f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now each element of this of shape context length x context length\n",
    "attention_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9129667f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0365,  0.4661, -0.2813,  0.0463, -0.1883, -0.2296, -0.4358, -0.2512],\n",
       "        [ 0.2019,  0.8453, -4.4513, -1.1401,  1.3999,  0.5916, -0.0298,  0.4806],\n",
       "        [ 0.3817, -0.9313, -1.1860,  0.9979,  0.5567,  0.3707,  0.1498,  0.8662],\n",
       "        [ 0.5086, -0.4611,  2.2789,  0.5995, -0.6054,  0.3011, -0.1403,  0.1271],\n",
       "        [ 0.2204,  0.3042,  1.1581, -0.4140, -0.3638,  0.0236, -0.0526, -0.3158],\n",
       "        [-2.6575, -0.7755, -0.7823,  1.7996, -0.2031, -1.4972, -0.0844, -0.2259],\n",
       "        [ 0.8128,  0.6337,  0.9232, -0.9524, -0.2063,  0.4751, -0.1114, -0.2157],\n",
       "        [ 1.1258,  0.3966,  0.4558, -0.6336,  0.0213,  0.8962, -0.2089,  0.2331]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f75c1",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "V`[i] = V[0]*A_softmax[i][0] + V[1]*A_softmax[i][1] + V[2]*A_softmax[i][2] + ...\n",
    "\n",
    "```\n",
    "\n",
    "now in self attention when we compute the change required for ith vector we don't want any token above ith token to be deciding how it changes, cause during inference we don't have that information at all so we would need to find a way so that above becomes\n",
    "\n",
    "```\n",
    "\n",
    "V`[i] = V[0]*A_softmax[i][0] + V[1]*A_softmax[i][1] + V[2]*A_softmax[i][2] + ... + V[i]*A_softmax[i][i]\n",
    "\n",
    "```\n",
    "\n",
    "so to do this we convert the lower triangular part equal infinity so that when we do softmax we get zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f623dd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21, 11, 23, 22, 24, 42, 30, 17])\n",
      "tensor([11, 23, 22, 24, 42, 30, 17,  2])\n"
     ]
    }
   ],
   "source": [
    "print(x_batch[0])\n",
    "print(y_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "625dc1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(attention_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7aeb0233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False],\n",
       "        [ True, False, False, False, False, False, False, False],\n",
       "        [ True,  True, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones_like(attention_matrix[0]), diagonal=-1).bool() # should the diagonal also have no contribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31611f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0365,  0.4661, -0.2813,  0.0463, -0.1883, -0.2296, -0.4358, -0.2512],\n",
       "        [   -inf,  0.8453, -4.4513, -1.1401,  1.3999,  0.5916, -0.0298,  0.4806],\n",
       "        [   -inf,    -inf, -1.1860,  0.9979,  0.5567,  0.3707,  0.1498,  0.8662],\n",
       "        [   -inf,    -inf,    -inf,  0.5995, -0.6054,  0.3011, -0.1403,  0.1271],\n",
       "        [   -inf,    -inf,    -inf,    -inf, -0.3638,  0.0236, -0.0526, -0.3158],\n",
       "        [   -inf,    -inf,    -inf,    -inf,    -inf, -1.4972, -0.0844, -0.2259],\n",
       "        [   -inf,    -inf,    -inf,    -inf,    -inf,    -inf, -0.1114, -0.2157],\n",
       "        [   -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,  0.2331]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_matrix[0].masked_fill(torch.tril(torch.ones_like(attention_matrix[0]), diagonal=-1).bool(), float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "773b3d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0365,  0.4661, -0.2813,  0.0463, -0.1883, -0.2296, -0.4358, -0.2512],\n",
       "        [   -inf,  0.8453, -4.4513, -1.1401,  1.3999,  0.5916, -0.0298,  0.4806],\n",
       "        [   -inf,    -inf, -1.1860,  0.9979,  0.5567,  0.3707,  0.1498,  0.8662],\n",
       "        [   -inf,    -inf,    -inf,  0.5995, -0.6054,  0.3011, -0.1403,  0.1271],\n",
       "        [   -inf,    -inf,    -inf,    -inf, -0.3638,  0.0236, -0.0526, -0.3158],\n",
       "        [   -inf,    -inf,    -inf,    -inf,    -inf, -1.4972, -0.0844, -0.2259],\n",
       "        [   -inf,    -inf,    -inf,    -inf,    -inf,    -inf, -0.1114, -0.2157],\n",
       "        [   -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,  0.2331]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_matrix.masked_fill(torch.tril(torch.ones_like(attention_matrix), diagonal=-1).bool(), float('-inf'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1292ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_attention_matrix = attention_matrix.masked_fill(torch.tril(torch.ones_like(attention_matrix), diagonal=-1).bool(), float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ca59759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0365,  0.4661, -0.2813,  0.0463, -0.1883, -0.2296, -0.4358, -0.2512],\n",
       "        [   -inf,  0.8453, -4.4513, -1.1401,  1.3999,  0.5916, -0.0298,  0.4806],\n",
       "        [   -inf,    -inf, -1.1860,  0.9979,  0.5567,  0.3707,  0.1498,  0.8662],\n",
       "        [   -inf,    -inf,    -inf,  0.5995, -0.6054,  0.3011, -0.1403,  0.1271],\n",
       "        [   -inf,    -inf,    -inf,    -inf, -0.3638,  0.0236, -0.0526, -0.3158],\n",
       "        [   -inf,    -inf,    -inf,    -inf,    -inf, -1.4972, -0.0844, -0.2259],\n",
       "        [   -inf,    -inf,    -inf,    -inf,    -inf,    -inf, -0.1114, -0.2157],\n",
       "        [   -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,  0.2331]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attention_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "918a1bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.4063, 0.7042, 0.1775, 0.1053, 0.1195, 0.1009, 0.0819],\n",
       "        [0.0000, 0.5937, 0.0109, 0.0542, 0.5153, 0.2717, 0.1514, 0.1702],\n",
       "        [0.0000, 0.0000, 0.2850, 0.4597, 0.2217, 0.2179, 0.1812, 0.2502],\n",
       "        [0.0000, 0.0000, 0.0000, 0.3086, 0.0694, 0.2032, 0.1356, 0.1195],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0883, 0.1540, 0.1480, 0.0767],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0337, 0.1434, 0.0839],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1396, 0.0848],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1328]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "F.softmax(masked_attention_matrix[0], dim=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7942367",
   "metadata": {},
   "source": [
    "### should the diagonal (i.e the current token) have any contribution towards update of input embedding vector?\n",
    "\n",
    "if it does then ``V`[0] = V[0]`` and that would mean the updated embedding would become ``E_updated[0] = V[0] + E[0]`` need to think more about it\n",
    "\n",
    "but according to my understanding `V[0]` is same as the absolute embedding of the token so `V[0] + E[0]` would be pracitically `2*absolute_mebdding[0]` since even `E[0]` was created any info about token after that so just itself, but since we don layer norm `norm(2*absolute_embedding[0]) = absolute_embedding[0]` so even if it scales a lot layer normalization brings down the scale of embedding vector so it fits with rest of embedding vector's scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69eb81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_attention_matrix = F.softmax(masked_attention_matrix, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a9e3d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.4063, 0.7042, 0.1775, 0.1053, 0.1195, 0.1009, 0.0819],\n",
       "        [0.0000, 0.5937, 0.0109, 0.0542, 0.5153, 0.2717, 0.1514, 0.1702],\n",
       "        [0.0000, 0.0000, 0.2850, 0.4597, 0.2217, 0.2179, 0.1812, 0.2502],\n",
       "        [0.0000, 0.0000, 0.0000, 0.3086, 0.0694, 0.2032, 0.1356, 0.1195],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0883, 0.1540, 0.1480, 0.0767],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0337, 0.1434, 0.0839],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1396, 0.0848],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1328]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_attention_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56d5dee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8822e+00,  8.1252e-01, -1.2195e+00,  1.9457e+00,  2.7619e-01,\n",
       "          7.7019e-01, -8.2986e-01, -1.9775e+00],\n",
       "        [ 3.0802e-01,  3.9843e-01, -1.0687e+00, -1.5616e+00,  7.8532e-01,\n",
       "         -9.1509e-01,  3.0454e-01,  5.3056e-02],\n",
       "        [ 1.5264e+00, -4.9802e-01, -2.9694e-02,  1.8806e-01,  1.7161e-01,\n",
       "          7.8767e-01, -9.2485e-01, -1.6506e+00],\n",
       "        [-8.1766e-01, -9.5586e-01,  1.2630e+00,  2.1786e-01, -5.7029e-01,\n",
       "          2.1527e-01, -2.3363e-01,  6.6448e-03],\n",
       "        [ 1.3550e-01,  3.1043e-01, -1.2017e-01,  5.3248e-01, -1.3246e-01,\n",
       "          2.1973e-03, -1.5388e-01, -4.8836e-02],\n",
       "        [-1.3120e-02,  7.2783e-01,  2.8863e-01,  1.3408e+00,  1.3093e+00,\n",
       "          1.7083e-01,  1.6660e-01, -7.6276e-01],\n",
       "        [ 1.0175e+00,  1.3398e-01,  4.7444e-01,  1.1533e+00, -8.3743e-01,\n",
       "          1.8518e+00, -4.0915e-01, -7.5429e-01],\n",
       "        [ 7.8381e-01,  7.6704e-02,  2.6689e-01,  7.3089e-01,  1.5441e-01,\n",
       "          2.1513e-01, -9.8640e-01, -1.4326e+00]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c725c899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1165,  0.5935, -1.3589,  1.8748,  0.6856,  1.2163, -1.5171, -3.4067],\n",
       "        [ 0.5088,  0.5704, -0.4326,  0.0245,  0.6242, -0.1585, -0.1057, -0.5765],\n",
       "        [ 0.4667, -0.3104,  0.7611,  0.9558, -0.0705,  0.7505, -0.6897, -1.1394],\n",
       "        [-0.0140, -0.0982,  0.5363,  0.6203, -0.0142,  0.3781, -0.2222, -0.4298],\n",
       "        [ 0.2207,  0.1652,  0.1245,  0.4802,  0.0778,  0.3171, -0.1242, -0.3433],\n",
       "        [ 0.2112,  0.0501,  0.1001,  0.2718, -0.0630,  0.2893, -0.1359, -0.2541],\n",
       "        [ 0.2085,  0.0252,  0.0888,  0.2229, -0.1038,  0.2767, -0.1407, -0.2268],\n",
       "        [ 0.1041,  0.0102,  0.0355,  0.0971,  0.0205,  0.0286, -0.1310, -0.1903]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_attention_matrix[0]@V[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9eecf1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_prime = batch_attention_matrix@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b0db6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_unnormalized = V_prime + x_embeddings_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b955be",
   "metadata": {},
   "source": [
    "# Layer normalization\n",
    "\n",
    "we just normalize across the embedding dimension to make sure this addition does not scale the embedding vector to much across each layer so that it has the scale of the original embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5530fd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_emebdding = embedding_unnormalized.mean(dim=-1)\n",
    "mean_emebdding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34d28a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_embedding = embedding_unnormalized.std(dim=-1)\n",
    "std_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0a5faad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_unnormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56b574a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2358, grad_fn=<SelectBackward0>) tensor(3.4888, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(mean_emebdding[0][0], std_embedding[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63446123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2030, -0.4839, -1.0330,  1.6250,  0.3421, -0.6380,  0.0268, -1.0420],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding_unnormalized[0][0] - mean_emebdding[0][0])/(std_embedding[0][0] + 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6d86339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2030, -0.4839, -1.0330,  1.6250,  0.3421, -0.6380,  0.0268, -1.0420],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_normalized = (embedding_unnormalized - mean_emebdding.unsqueeze(-1))/(std_embedding.unsqueeze(-1) + 0.0001) # unsqueeze needed to broadcast\n",
    "embedding_normalized[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47933165",
   "metadata": {},
   "source": [
    "# Multi head self attention\n",
    "\n",
    "![](20251130180338.png)\n",
    "\n",
    "Functionaly what multi headed attention does is, it employs multiple smaller self attention blocks, the input to each is the full embedding vectors, and the output are vectors with a smaller dimension\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "let's say the input is [6, 6, 6]  (here 6 represents the dimension of the embedding vector which goes in, and 3 is the context length)\n",
    "\n",
    "\n",
    "[6, 6, 6] -> head 1 -> [2, 2, 2]'1 (here I am just using '1 as a label)\n",
    "\n",
    "[6, 6, 6] -> head 2 -> [2, 2, 2]'2\n",
    "\n",
    "[6, 6, 6] -> head 3 -> [2, 2, 2]'3\n",
    "\n",
    "\n",
    "concat([2, 2, 2]'1, [2, 2, 2]'2, [2, 2, 2]'3) -> [6, 6, 6]'\n",
    "\n",
    "[6, 6, 6]' -> linear layer -> [6, 6, 6]'' --> residual connection --> [6, 6, 6]'' + [6, 6, 6] --> layer norm --> [6, 6, 6]final\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee29868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing everything we have done in one block\n",
    "\n",
    "# ----multi head repetition--------\n",
    "d_k = d_q = 4\n",
    "d_v = EMBEDDING_DIMENSION\n",
    "\n",
    "W_q = nn.Linear(EMBEDDING_DIMENSION, d_q, bias = False)\n",
    "W_k = nn.Linear(EMBEDDING_DIMENSION, d_k, bias = False)\n",
    "W_v = nn.Linear(EMBEDDING_DIMENSION, d_v, bias = False) # for single head attention this needs to be same as embedding dimension cause the resulting vector of attention layer get added to input embedding vector so both need to have same dimension\n",
    "\n",
    "Q, K, V = W_q(x_embeddings_total), W_k(x_embeddings_total), W_v(x_embeddings_total)\n",
    "\n",
    "attention_matrix = Q@K.transpose(1, 2) # transpose the 1st and the 2nd dimension not the  this is equivalent to Q[i]@K[i].transpose where i is each element of the batch\n",
    "\n",
    "masked_attention_matrix = attention_matrix.masked_fill(torch.tril(torch.ones_like(attention_matrix), diagonal=-1).bool(), float('-inf'))\n",
    "\n",
    "batch_attention_matrix = F.softmax(masked_attention_matrix, dim=-2)\n",
    "\n",
    "V_prime = batch_attention_matrix@V\n",
    "# ----multi head repetition--------\n",
    "\n",
    "embedding_unnormalized = V_prime + x_embeddings_total\n",
    "\n",
    "mean_emebdding = embedding_unnormalized.mean(dim=-1)\n",
    "std_embedding = embedding_unnormalized.std(dim=-1)\n",
    "\n",
    "embedding_normalized = (embedding_unnormalized - mean_emebdding.unsqueeze(-1))/(std_embedding.unsqueeze(-1) + 0.0001) # unsqueeze needed to broadcast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3865bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let me create a class out of the repeated block\n",
    "\n",
    "class SingleHead():\n",
    "\n",
    "    def __init__(self, d_k, d_q, d_v, input_dimension) -> None:\n",
    "        \n",
    "        self.W_q = nn.Linear(input_dimension, d_q, bias = False)\n",
    "        self.W_k = nn.Linear(input_dimension, d_k, bias = False)\n",
    "        self.W_v = nn.Linear(input_dimension, d_v, bias = False) # for single head attention this needs to be same as embedding dimension cause the resulting vector of attention layer get added to input embedding vector so both need to have same dimension\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_embeddings):\n",
    "        Q, K, V = self.W_q(input_embeddings), self.W_k(input_embeddings), self.W_v(input_embeddings)\n",
    "\n",
    "        attention_matrix = Q@K.transpose(-2, -1) # transpose the 1st and the 2nd dimension not the  this is equivalent to Q[i]@K[i].transpose where i is each element of the batch\n",
    "\n",
    "        masked_attention_matrix = attention_matrix.masked_fill(torch.tril(torch.ones_like(attention_matrix), diagonal=-1).bool(), float('-inf'))\n",
    "\n",
    "        batch_attention_matrix = F.softmax(masked_attention_matrix, dim=-2)\n",
    "\n",
    "        V_prime = batch_attention_matrix@V\n",
    "\n",
    "        return V_prime\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ac9a7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 8])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embeddings_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0cffb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our embedding is of size 8 I am gonna create 2 heads with output dimensino 4\n",
    "\n",
    "head1, head2 = SingleHead(2, 2, 4, 8), SingleHead(2, 2, 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd0881e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0737, -1.9968,  0.8206,  0.1649],\n",
      "        [-0.4561, -0.4630, -0.3824,  0.5564],\n",
      "        [-0.1686, -0.4715,  0.3744,  0.5112],\n",
      "        [-0.2452, -0.2064,  0.8486,  0.0039],\n",
      "        [-0.1601, -0.2369,  0.4407, -0.1236],\n",
      "        [ 0.0022, -0.2633,  0.2046, -0.0096],\n",
      "        [ 0.0198, -0.1867,  0.1099, -0.0108],\n",
      "        [-0.1292, -0.1143,  0.0695, -0.0233]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-1.9296, -0.2495,  0.5440,  1.8510],\n",
      "        [-0.3440,  0.6963,  1.1398,  0.5546],\n",
      "        [ 0.6122,  0.3662,  0.8504, -0.0491],\n",
      "        [-0.0973,  1.1241,  0.6411,  1.3044],\n",
      "        [ 0.0126,  0.2975,  0.1584,  0.3997],\n",
      "        [-0.0123,  0.2337,  0.1595,  0.2554],\n",
      "        [ 0.0282,  0.0328,  0.0433,  0.0269],\n",
      "        [ 0.0446,  0.0544,  0.0691,  0.0485]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0737, -1.9968,  0.8206,  0.1649, -1.9296, -0.2495,  0.5440,  1.8510],\n",
      "        [-0.4561, -0.4630, -0.3824,  0.5564, -0.3440,  0.6963,  1.1398,  0.5546],\n",
      "        [-0.1686, -0.4715,  0.3744,  0.5112,  0.6122,  0.3662,  0.8504, -0.0491],\n",
      "        [-0.2452, -0.2064,  0.8486,  0.0039, -0.0973,  1.1241,  0.6411,  1.3044],\n",
      "        [-0.1601, -0.2369,  0.4407, -0.1236,  0.0126,  0.2975,  0.1584,  0.3997],\n",
      "        [ 0.0022, -0.2633,  0.2046, -0.0096, -0.0123,  0.2337,  0.1595,  0.2554],\n",
      "        [ 0.0198, -0.1867,  0.1099, -0.0108,  0.0282,  0.0328,  0.0433,  0.0269],\n",
      "        [-0.1292, -0.1143,  0.0695, -0.0233,  0.0446,  0.0544,  0.0691,  0.0485]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_out_head_1 = head1.forward(x_embeddings_total)[0]\n",
    "print(x_out_head_1)\n",
    "x_out_head_2 = head2.forward(x_embeddings_total)[0]\n",
    "print(x_out_head_2)\n",
    "print(torch.cat([x_out_head_1, x_out_head_2], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5417a457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0737, -1.9968,  0.8206,  0.1649, -1.9296, -0.2495,  0.5440,  1.8510],\n",
       "        [-0.4561, -0.4630, -0.3824,  0.5564, -0.3440,  0.6963,  1.1398,  0.5546],\n",
       "        [-0.1686, -0.4715,  0.3744,  0.5112,  0.6122,  0.3662,  0.8504, -0.0491],\n",
       "        [-0.2452, -0.2064,  0.8486,  0.0039, -0.0973,  1.1241,  0.6411,  1.3044],\n",
       "        [-0.1601, -0.2369,  0.4407, -0.1236,  0.0126,  0.2975,  0.1584,  0.3997],\n",
       "        [ 0.0022, -0.2633,  0.2046, -0.0096, -0.0123,  0.2337,  0.1595,  0.2554],\n",
       "        [ 0.0198, -0.1867,  0.1099, -0.0108,  0.0282,  0.0328,  0.0433,  0.0269],\n",
       "        [-0.1292, -0.1143,  0.0695, -0.0233,  0.0446,  0.0544,  0.0691,  0.0485]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_out_embedding_total = torch.cat([head1.forward(x_embeddings_total), head2.forward(x_embeddings_total)], dim= -1)\n",
    "x_out_embedding_total[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70d47348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add and norm \n",
    "\n",
    "embedding_unnormalized = x_out_embedding_total + x_embeddings_total\n",
    "mean_emebdding = embedding_unnormalized.mean(dim=-1)\n",
    "std_embedding = embedding_unnormalized.std(dim=-1)\n",
    "embedding_normalized = (embedding_unnormalized - mean_emebdding.unsqueeze(-1))/(std_embedding.unsqueeze(-1) + 0.0001) # unsqueeze needed to broadcast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a520c7c",
   "metadata": {},
   "source": [
    "Now that I have shown functionaly how multi head attention works we need to understand what it really means\n",
    "\n",
    "it is really like head 1 decides from 0 to 3rd direction which direction it should move in and head 2 decide from 4th to 7th which direction embedding should move in\n",
    "\n",
    "why we concat here ? why we not add them up? this I did not understand conceptually when I think of multi heads adding up, then I could argue that each head might have its own interpretation of how the space looks like which would might cause contradictory changes to the embeddings, another way to think of it is that adding all of them up would lead to a explotion in the value for each embedding dimension, tho layer norm should fix that\n",
    "\n",
    "this still needs to be debated, I will have to think more deeply about what it means to add them up vs concat them\n",
    "\n",
    "ok so it turns out I may have missed the feed forward step which has to be supposedly after every multi head attention layer\n",
    "\n",
    "So the flow should be\n",
    "\n",
    "1. Multi-Head self attention\n",
    "2. Add residual\n",
    "3. Layer norm\n",
    "4. feed forward\n",
    "5. add residual\n",
    "6. layer norm\n",
    "\n",
    "in my above example I missed from 3, so let me add that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "661438e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(EMBEDDING_DIMENSION, EMBEDDING_DIMENSION)\n",
    "\n",
    "embedding_mixed = linear_layer(embedding_normalized) # embedding_normalized vector supposedly have all the information in bits and piece 0 to 3rd dim comes from head 1 3rd to 6th from another and so on, \n",
    "# the embedding_mixed is supposed to have the mixed info about all these layers\n",
    "embedding_mixed = embedding_normalized + embedding_mixed # residual connection\n",
    "mean_embedding_mixed = embedding_mixed.mean(dim=-1)\n",
    "std_embedding_mixed = embedding_mixed.std(dim=-1)\n",
    "\n",
    "embedding_mixed_normalized = (embedding_mixed - mean_embedding_mixed.unsqueeze(-1))/(std_embedding_mixed.unsqueeze(-1) + 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dacff8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2223, -0.6036, -0.2103,  1.7107, -0.2706, -1.6552,  0.7229,  0.5283],\n",
       "        [-0.7689,  0.3730, -1.0578, -0.0193,  1.3909,  0.3113,  1.1285, -1.3577],\n",
       "        [-0.7461, -0.1377, -0.5951,  1.6825, -0.3437, -1.0435,  1.4075, -0.2239],\n",
       "        [ 0.3722, -0.7045, -0.2144, -0.9540, -0.6403,  1.3113, -0.7962,  1.6260],\n",
       "        [-0.4414, -1.1687,  1.4907,  0.1020,  0.7014, -1.0049, -0.7485,  1.0693],\n",
       "        [-1.8770, -0.1055,  0.0631,  0.4470,  1.2735, -0.7064, -0.1516,  1.0569],\n",
       "        [-0.0672, -0.5733, -0.3826,  1.9516, -0.6731, -0.3870,  1.0903, -0.9586],\n",
       "        [-0.2752, -1.6222, -0.0931,  0.7374,  0.4912, -0.8739, -0.0175,  1.6533]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_mixed_normalized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556486d",
   "metadata": {},
   "source": [
    "Ok now this constitutes of the full multihead attention block, I still feel like the explanation:\n",
    "\"Each head computes its own bits of feature that it understands, and then the feed forward layer after that combines and picks and mixes the relevant info to form a refined vector\" still a bit hand wavy I might need to actually see what this layer is doing, for that I will have to analyze a already existing pretrained model, which would be the next thing I will do, since I don't have money to buy GPUs I will have to rely on analyzing already trained models, let's continue with next stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d15edd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the existing embedding vectors into existing vocab space\n",
    "\n",
    "project_to_vocab_layer = nn.Linear(EMBEDDING_DIMENSION, len(vocabulary_list))\n",
    "\n",
    "logits = project_to_vocab_layer(embedding_mixed_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b01a792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 44])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0].shape # context length, vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7cf98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploding logits and expected value for easy cross entropy computation\n",
    "\n",
    "logits_exploded = logits.view(logits.shape[0]*logits.shape[1], logits.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d82864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_exploded = y_batch.view(y_batch.shape[0]*y_batch.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f64f6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7880, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits_exploded,targets_exploded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcbe35",
   "metadata": {},
   "source": [
    "finally completed one single pass through transformers layers, next I will create modules out of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c8a63",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
