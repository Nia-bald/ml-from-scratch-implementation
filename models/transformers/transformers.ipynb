{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b763ccb7",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb6e64",
   "metadata": {},
   "source": [
    "architecture screentshot:\n",
    "\n",
    "![](20251120024008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a0e61",
   "metadata": {},
   "source": [
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f86cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "training_data = list(\"\"\"\n",
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset\n",
    "\n",
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d441707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't want to get too deep into tokenization for this notebook so I am just going to instead use all the unique characters\n",
    "# present in the training data as distinct tokens\n",
    "vocabulary_list = list(set(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57b7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'd', 'e', 'v', ']']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_list[:5])\n",
    "print(len(vocabulary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a3b20e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f']\n"
     ]
    }
   ],
   "source": [
    "# let's create training and testing data\n",
    "# training and testing data for next token prediction would look something like\n",
    "\n",
    "# the way the transformer works is that for a single example sentence it trains the model for multiple token prediction\n",
    "print(training_data[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565ee083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here if x is\n",
    "training_data[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0271b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', ' ', 'T', 'r', 'a', 'n', 's', 'f']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then y would be\n",
    "training_data[1:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34721d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok before we make create training data we need to convert our tokens to a unique index to do that I will do\n",
    "token_to_index = {c:i for i,c in enumerate(vocabulary_list)}\n",
    "index_to_token = {i:c for i,c in enumerate(vocabulary_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we let's convert our training data to a torch tensor\n",
    "import torch\n",
    "\n",
    "training_data_tensor = torch.tensor([token_to_index[c] for c in training_data], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a91543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31, 35, 19, 11, 15, 33, 39,  8, 29, 22])\n",
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f', 'o']\n"
     ]
    }
   ],
   "source": [
    "print(training_data_tensor[:10])\n",
    "print([index_to_token[ix.item()] for ix in training_data_tensor[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3c920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create training and testing set\n",
    "block_size = 8\n",
    "x = torch.stack([training_data_tensor[ix:ix+block_size] for ix in range(len(training_data_tensor)-block_size)] )\n",
    "# max ix len(training_data_tensor)-block_size - 1\n",
    "# so ix + block_size = len(training_data_tensor) - 1\n",
    "# so final example won't include last character\n",
    "y = torch.stack([training_data_tensor[ix:ix+block_size]for ix in range(1,len(training_data_tensor)-block_size+1)]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc9e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x training data\n",
      "tensor([[31, 35, 19, 11, 15, 33, 39,  8],\n",
      "        [35, 19, 11, 15, 33, 39,  8, 29],\n",
      "        [19, 11, 15, 33, 39,  8, 29, 22],\n",
      "        [11, 15, 33, 39,  8, 29, 22, 15],\n",
      "        [15, 33, 39,  8, 29, 22, 15,  7]])\n",
      "y training data\n",
      "tensor([[35, 19, 11, 15, 33, 39,  8, 29],\n",
      "        [19, 11, 15, 33, 39,  8, 29, 22],\n",
      "        [11, 15, 33, 39,  8, 29, 22, 15],\n",
      "        [15, 33, 39,  8, 29, 22, 15,  7],\n",
      "        [33, 39,  8, 29, 22, 15,  7,  2]])\n"
     ]
    }
   ],
   "source": [
    "print(\"x training data\")\n",
    "print(x[:5])\n",
    "print(\"y training data\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3e38d",
   "metadata": {},
   "source": [
    "# Embedding Table\n",
    "\n",
    "![](20251121001141.png)\n",
    "\n",
    "\n",
    "This is a look up table between the vocabulary index and n dimensional vector,\n",
    "during the training of transformer model this vectors also gets trained, i.e where these vectors point to gets updated,\n",
    "based on the similarity between these vectors, if let's say I have 2 tokens \"dog\" and \"pooch\", during the start of training process\n",
    "they might point in very different directions, but after the training both would point to pretty much same place\n",
    "\n",
    "### Question?:\n",
    "\n",
    "1. What is so special about the training process that transforms these vectors from pointing in random ass direction, to actually have some meaning\n",
    "    * for now I am gonna assume that the answer is that the transformer architecture expects and assumes these vectors to be what I have described\n",
    "    * and based on this assumption, the subsequent layers performs its operation, so optimizing the loss leads to these embedding vector looking more like actual high dimensional representation of the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "644e2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "EMBEDDING_DIMENSION = 8\n",
    "VOCAB_SIZE = len(vocabulary_list)\n",
    "\n",
    "embeddings_table = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8c861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0414, -0.5294,  1.5743, -1.3742,  0.7060,  0.8735, -1.9587,\n",
      "           0.5864],\n",
      "         [-0.0100, -0.5723,  0.3159,  0.5923,  1.0571,  1.1296, -0.4674,\n",
      "          -0.4062],\n",
      "         [-1.1467, -1.6962, -1.9041, -0.6015, -0.3402, -0.4077,  1.4213,\n",
      "          -1.2096],\n",
      "         [-1.0485, -0.1236, -0.1045,  0.3219, -1.3044, -0.4208,  0.0385,\n",
      "           0.1210]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# some experimentation on how embeddings table work,\n",
    "print(embeddings_table(torch.tensor([[0,1,2,3]], dtype=torch.long)))\n",
    "# it goes to each item in tensor and assumes each item is a index converts it to its corresponding embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509c970",
   "metadata": {},
   "source": [
    "I want to do a very simple forward pass so I am gonna create my forward pass batch now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff721a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = x[:5]\n",
    "y_batch = y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfeba10",
   "metadata": {},
   "source": [
    "A question lingers, what does this (shifted right) mean:\n",
    "\n",
    "![](20251121002201.png)\n",
    "\n",
    "this just means that our input is shifted from the target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2beaa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings = embeddings_table(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f7c346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9303,  1.2898, -0.3014,  0.6136, -0.1183, -0.8329, -0.5625,\n",
       "          -1.6956],\n",
       "         [ 0.2655, -0.0230, -0.4772,  0.5039,  0.8260,  0.2511,  0.7968,\n",
       "           0.1618],\n",
       "         [-0.6083, -0.9330,  1.7121, -1.4437,  0.2876, -0.6583,  0.4152,\n",
       "          -0.2459],\n",
       "         [ 0.2318, -0.5281,  0.1381,  2.0172, -0.6610,  0.3451, -0.0356,\n",
       "          -0.1227],\n",
       "         [-1.8868,  0.6379,  0.2046, -0.2392,  0.5712, -0.5834,  0.0711,\n",
       "           1.5001],\n",
       "         [ 0.2956, -0.9214, -0.8951,  0.5017, -0.4490,  1.2130, -1.6055,\n",
       "          -0.3325],\n",
       "         [ 1.9535,  0.5234,  0.0406,  0.0649, -0.7019,  1.5298, -0.1788,\n",
       "          -0.8738],\n",
       "         [ 0.4258,  0.8844, -0.5111, -1.9000,  0.1877, -1.1666,  0.6706,\n",
       "           0.3286]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just one example\n",
    "x_embeddings[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6ad78",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "![](20251121135418.png)\n",
    "\n",
    "\n",
    "From my past understanding this is sort of values with varies with the position of the token in the sequence to encode the information about the position of the token in the sequence\n",
    "\n",
    "so for each position there will be a vector associated to it, which will get added to the original embedding vector at that position\n",
    "\n",
    "### Questions?:\n",
    "1. Why Add these vectors to the original embedding vector? Can it not be appended or create some other type of encoding create a new channel perhaps like we do for CNNs\n",
    "    - Ans: The Idea behind adding these is how we treat embedding vectors, you can think of embedding vector as the original absolute meaning of a token, now depending on whether it appears at the beggining of a sentence or end of a sentence it's meaning might differ, i.e its embedding vector might change its position, that change is capture by the addition of this positional embeddding vector\n",
    "2. Why do these needs to be a vector all together can these not be like a single number which gets added?\n",
    "    - Ans: well a vector is a more generalized version of a single number, if single number is the right approach then expectation is that the network would train the embedings to become a single number\n",
    "\n",
    "## Sinusoidal Encoding\n",
    "\n",
    "![](20251121140214.png)\n",
    "\n",
    "here d_model is the dimension of the embedding\n",
    "\n",
    "In the original Paper they used a fix positional sinusoidal encoding, they mentioned the performance for both learned and not learned were identical, they wanted to experiment with sinusoidal encoding, because they wanted to test the model beyond the trained context length\n",
    "\n",
    "# Question?:\n",
    "1. But why sinusoidal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810c2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embedding_table = nn.Embedding(block_size, EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e839a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpositional_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Machine_learning\\MLfromScratchImplementation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Machine_learning\\MLfromScratchImplementation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Machine_learning\\MLfromScratchImplementation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Machine_learning\\MLfromScratchImplementation\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f61c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
