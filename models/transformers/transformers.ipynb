{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b763ccb7",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb6e64",
   "metadata": {},
   "source": [
    "architecture screentshot:\n",
    "\n",
    "![](20251120024008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14a0e61",
   "metadata": {},
   "source": [
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f86cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "training_data = list(\"\"\"\n",
    "# Transformers\n",
    "\n",
    "this note book is here to help me refresh some of my understanding of the basic transformers architecture\n",
    "\n",
    "we want to implement the encoder part of the architecture in [attention is all you need paper](https://arxiv.org/pdf/1706.03762):\n",
    "\n",
    "My goal with be to go through one pass of transformer layer for a data, and try to explain each layer, finally I will convert this jupyter notebook to a python code and train it on a simple dataset\n",
    "\n",
    "# I want this note book to be very simple so I will make the data very simple, i.e use whatever I have written till now as training data\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d441707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't want to get too deep into tokenization for this notebook so I am just going to instead use all the unique characters\n",
    "# present in the training data as distinct tokens\n",
    "vocabulary_list = list(set(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57b7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', ':', 'n', 'I', '/']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_list[:5])\n",
    "print(len(vocabulary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a3b20e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f']\n"
     ]
    }
   ],
   "source": [
    "# let's create training and testing data\n",
    "# training and testing data for next token prediction would look something like\n",
    "\n",
    "# the way the transformer works is that for a single example sentence it trains the model for multiple token prediction\n",
    "print(training_data[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565ee083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here if x is\n",
    "training_data[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0271b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', ' ', 'T', 'r', 'a', 'n', 's', 'f']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then y would be\n",
    "training_data[1:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34721d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok before we make create training data we need to convert our tokens to a unique index to do that I will do\n",
    "token_to_index = {c:i for i,c in enumerate(vocabulary_list)}\n",
    "index_to_token = {i:c for i,c in enumerate(vocabulary_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc811d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we let's convert our training data to a torch tensor\n",
    "import torch\n",
    "\n",
    "training_data_tensor = torch.tensor([token_to_index[c] for c in training_data], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a91543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28, 24, 25, 40, 12,  7,  2, 19, 18, 42])\n",
      "['\\n', '#', ' ', 'T', 'r', 'a', 'n', 's', 'f', 'o']\n"
     ]
    }
   ],
   "source": [
    "print(training_data_tensor[:10])\n",
    "print([index_to_token[ix.item()] for ix in training_data_tensor[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3c920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create training and testing set\n",
    "block_size = 8\n",
    "x = torch.stack([training_data_tensor[ix:ix+block_size] for ix in range(len(training_data_tensor)-block_size)] )\n",
    "# max ix len(training_data_tensor)-block_size - 1\n",
    "# so ix + block_size = len(training_data_tensor) - 1\n",
    "# so final example won't include last character\n",
    "y = torch.stack([training_data_tensor[ix:ix+block_size]for ix in range(1,len(training_data_tensor)-block_size+1)]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc9e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x training data\n",
      "tensor([[28, 24, 25, 40, 12,  7,  2, 19],\n",
      "        [24, 25, 40, 12,  7,  2, 19, 18],\n",
      "        [25, 40, 12,  7,  2, 19, 18, 42],\n",
      "        [40, 12,  7,  2, 19, 18, 42, 12],\n",
      "        [12,  7,  2, 19, 18, 42, 12, 16]])\n",
      "y training data\n",
      "tensor([[24, 25, 40, 12,  7,  2, 19, 18],\n",
      "        [25, 40, 12,  7,  2, 19, 18, 42],\n",
      "        [40, 12,  7,  2, 19, 18, 42, 12],\n",
      "        [12,  7,  2, 19, 18, 42, 12, 16],\n",
      "        [ 7,  2, 19, 18, 42, 12, 16, 29]])\n"
     ]
    }
   ],
   "source": [
    "print(\"x training data\")\n",
    "print(x[:5])\n",
    "print(\"y training data\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3e38d",
   "metadata": {},
   "source": [
    "# Embedding Table\n",
    "\n",
    "![](20251121001141.png)\n",
    "\n",
    "\n",
    "This is a look up table between the vocabulary index and n dimensional vector,\n",
    "during the training of transformer model this vectors also gets trained, i.e where these vectors point to gets updated,\n",
    "based on the similarity between these vectors, if let's say I have 2 tokens \"dog\" and \"pooch\", during the start of training process\n",
    "they might point in very different directions, but after the training both would point to pretty much same place\n",
    "\n",
    "### Question?:\n",
    "\n",
    "1. What is so special about the training process that transforms these vectors from pointing in random ass direction, to actually have some meaning\n",
    "    * for now I am gonna assume that the answer is that the transformer architecture expects and assumes these vectors to be what I have described\n",
    "    * and based on this assumption, the subsequent layers performs its operation, so optimizing the loss leads to these embedding vector looking more like actual high dimensional representation of the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "644e2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "EMBEDDING_DIMENSION = 8\n",
    "VOCAB_SIZE = len(vocabulary_list)\n",
    "\n",
    "embeddings_table = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8c861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3714,  0.8524,  1.5221, -1.0615, -0.3488,  0.7318,  2.0752,\n",
      "           0.4439],\n",
      "         [ 0.3213,  0.5684,  1.0322, -1.5972, -1.5429,  1.0081,  0.8304,\n",
      "           0.3172],\n",
      "         [-1.5154,  0.3275,  0.8591,  0.2626,  1.4236,  0.4716, -0.7320,\n",
      "           0.2738],\n",
      "         [-0.0512, -1.6777, -1.0690, -0.1452,  1.2174,  0.5099, -0.1739,\n",
      "           0.0405]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# some experimentation on how embeddings table work,\n",
    "print(embeddings_table(torch.tensor([[0,1,2,3]], dtype=torch.long)))\n",
    "# it goes to each item in tensor and assumes each item is a index converts it to its corresponding embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509c970",
   "metadata": {},
   "source": [
    "I want to do a very simple forward pass so I am gonna create my forward pass batch now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff721a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch = x[:5]\n",
    "y_batch = y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfeba10",
   "metadata": {},
   "source": [
    "A question lingers, what does this (shifted right) mean:\n",
    "\n",
    "![](20251121002201.png)\n",
    "\n",
    "this just means that our input is shifted from the target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2beaa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embeddings = embeddings_table(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8f7c346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9814, -0.0656,  0.0673,  0.5413,  1.1494, -1.3388,  0.2755,\n",
       "           0.7445],\n",
       "         [ 0.7848, -0.2118,  0.9142, -0.0479, -0.8624,  0.6424,  0.2042,\n",
       "          -0.2691],\n",
       "         [-0.0411, -0.9826,  0.0639,  0.2972, -0.8725, -1.1352,  0.3890,\n",
       "           1.6370],\n",
       "         [-0.8569,  1.3421,  0.2077, -0.0831, -1.7176,  0.6414, -0.1823,\n",
       "          -0.8655],\n",
       "         [ 1.2021, -0.4688, -0.5701, -1.3151,  0.4430,  0.5387,  0.0251,\n",
       "           0.4245],\n",
       "         [-1.1869, -1.4327, -0.0363,  1.5950, -0.4343, -2.0443,  1.6574,\n",
       "          -0.3617],\n",
       "         [-1.5154,  0.3275,  0.8591,  0.2626,  1.4236,  0.4716, -0.7320,\n",
       "           0.2738],\n",
       "         [ 0.3850,  0.5533, -0.3635, -1.2622, -1.1007, -0.7654,  0.2875,\n",
       "           0.4457]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just one example\n",
    "x_embeddings[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6ad78",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "![](20251121135418.png)\n",
    "\n",
    "\n",
    "From my past understanding this is sort of values with varies with the position of the token in the sequence to encode the information about the position of the token in the sequence\n",
    "\n",
    "so for each position there will be a vector associated to it, which will get added to the original embedding vector at that position\n",
    "\n",
    "### Questions?:\n",
    "1. Why Add these vectors to the original embedding vector? Can it not be appended or create some other type of encoding create a new channel perhaps like we do for CNNs\n",
    "    - Ans: The Idea behind adding these is how we treat embedding vectors, you can think of embedding vector as the original absolute meaning of a token, now depending on whether it appears at the beggining of a sentence or end of a sentence it's meaning might differ, i.e its embedding vector might change its position, that change is capture by the addition of this positional embeddding vector\n",
    "2. Why do these needs to be a vector all together can these not be like a single number which gets added?\n",
    "    - Ans: well a vector is a more generalized version of a single number, if single number is the right approach then expectation is that the network would train the embedings to become a single number\n",
    "\n",
    "## Sinusoidal Encoding\n",
    "\n",
    "![](20251121140214.png)\n",
    "\n",
    "here d_model is the dimension of the embedding\n",
    "\n",
    "In the original Paper they used a fix positional sinusoidal encoding, they mentioned the performance for both learned and not learned were identical, they wanted to experiment with sinusoidal encoding, because they wanted to test the model beyond the trained context length\n",
    "\n",
    "# Question?:\n",
    "1. But why sinusoidal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810c2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embedding_table = nn.Embedding(block_size, EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6e839a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos_embeddings = positional_embedding_table(torch.arange(x_embeddings.shape[1])) # C, E\n",
    "\n",
    "x_embeddings # B, C, E\n",
    "\n",
    "x_embeddings_total = x_embeddings + x_pos_embeddings # B, C, E + C, E -> pytorch checks the shape starting from right and if there is an extra dimension it creates a new dimention and copuies the same thing over, like C, E -> (1, C, E) -> (B, C, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f61c12",
   "metadata": {},
   "source": [
    "# Self Attention Layer\n",
    "\n",
    "![](20251123232032.png)\n",
    "\n",
    "I will start of by explaining what this layer does in a high level, then I will dig deep into how it does this, initially I will go over a single head self attention, \n",
    "then understand myself and explain why multi head self attention\n",
    "\n",
    "this is the 3b1b interpretation of this layer on a high level, which I found to be the most elegant\n",
    "\n",
    "## The Explanation\n",
    "This layer as a whole tells us how should the original embedding vector be modified, so that it's meaning is enriched with the context of the surrounding tokens, for example take the sentence:\n",
    "\n",
    "\" That blue aeroplane is very dangerous \"\n",
    "\n",
    "in this example initially \"aeroplane\"'s embedding vector would straight up point to the absolute aeroplane,\n",
    "then attention layer outputs a result, that result when added to the original embedding vector, nudges the aeroplane's vector in a direction closer to blue and dangerous\n",
    "\n",
    "that is on high level what this layer does, now going into the detail let's start by the equation\n",
    "\n",
    "![](20251123233808.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f5c08",
   "metadata": {},
   "source": [
    "the above represent the equation describing the self attention mechanism, for the purpose of this excercise we will focus on masked self attention\n",
    "\n",
    "here Q, K, V are all matrices\n",
    "\n",
    "Q being the query matrix, K Key matrix and V value matrix\n",
    "\n",
    "let me do one thing and form this forumla in our on going example and then explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a4dff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = d_q = d_v = 10\n",
    "\n",
    "W_q = nn.Linear(EMBEDDING_DIMENSION, d_q, bias = False)\n",
    "W_k = nn.Linear(EMBEDDING_DIMENSION, d_k, bias = False)\n",
    "W_v = nn.Linear(EMBEDDING_DIMENSION, d_v, bias = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9a8f7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1780a725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 8])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embeddings_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "039e3642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7016, -1.2739, -0.0510, -1.7870,  0.1321, -0.3128,  0.2754, -0.1269,\n",
       "         0.8233, -0.8119], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.weight@x_embeddings_total[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e849c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.0161e-01, -1.2739e+00, -5.1022e-02, -1.7870e+00,  1.3210e-01,\n",
       "          -3.1276e-01,  2.7537e-01, -1.2691e-01,  8.2333e-01, -8.1188e-01],\n",
       "         [ 6.0421e-01,  2.4767e-02, -7.9699e-01,  2.3967e-01, -3.9853e-01,\n",
       "           7.0460e-01,  9.3921e-01,  1.2793e+00,  1.2973e+00,  9.4875e-01],\n",
       "         [ 4.3131e-01, -5.0135e-02,  3.3971e-01, -2.6624e-02,  1.0610e+00,\n",
       "          -5.5387e-01, -3.4110e-01, -4.1804e-01, -3.9566e-03,  7.0141e-01],\n",
       "         [-1.1012e-01,  5.2924e-01,  5.2885e-01,  6.2617e-01,  4.3642e-01,\n",
       "          -7.2202e-02, -7.8692e-01, -5.5516e-01, -1.5537e+00,  1.0767e-01],\n",
       "         [-9.8368e-01,  2.0109e-01, -1.7731e-01, -3.4423e-01, -1.5610e+00,\n",
       "           1.6362e+00,  3.5768e-01, -2.6799e-01, -3.1720e-01, -7.3949e-01],\n",
       "         [-5.2773e-01,  2.4939e-01,  1.1789e+00, -3.0693e-02,  1.0144e+00,\n",
       "          -1.8045e+00,  2.2515e-01,  3.8290e-01,  2.5219e-01,  1.7592e+00],\n",
       "         [ 6.2366e-01,  2.8153e-01,  9.3578e-01,  7.8686e-01, -6.3206e-01,\n",
       "          -7.9035e-01,  5.2336e-01, -4.9448e-01, -1.4613e-01, -1.0493e+00],\n",
       "         [-2.4564e+00,  7.8665e-04, -1.3739e+00, -1.6848e+00, -7.8716e-01,\n",
       "           1.3793e+00, -1.1035e+00, -5.1059e-01, -2.0007e-01, -6.3132e-01]],\n",
       "\n",
       "        [[ 2.6594e-01, -7.7292e-01, -5.8224e-01, -1.0948e+00,  5.2043e-01,\n",
       "           2.7686e-01, -6.1003e-02, -1.7287e-01,  3.9973e-01, -4.9814e-01],\n",
       "         [ 1.6103e-01, -2.4612e-01, -9.8231e-01, -6.2550e-01, -5.2875e-01,\n",
       "           7.3716e-01,  8.9114e-01,  7.5098e-01,  1.8067e+00,  4.6540e-01],\n",
       "         [ 6.2288e-01,  8.3230e-01,  6.2221e-01,  8.0949e-01,  1.4166e+00,\n",
       "          -8.4401e-01, -9.0851e-01, -6.9047e-01, -1.3262e+00,  8.5808e-01],\n",
       "         [-7.1024e-01, -4.8295e-01,  3.8667e-01, -6.5578e-04, -9.3777e-01,\n",
       "           8.1981e-01,  5.2462e-01,  5.4029e-01, -2.1879e-01, -1.2486e-01],\n",
       "         [-5.5525e-01,  1.1640e+00,  7.5720e-01,  3.3712e-01,  2.8107e-02,\n",
       "          -4.6487e-02, -4.8230e-01, -1.1714e+00, -1.0147e+00,  4.6301e-01],\n",
       "         [ 4.7877e-01,  6.7764e-02,  1.2421e+00,  7.3184e-01, -4.5882e-02,\n",
       "          -1.3507e+00,  9.7674e-01,  5.7657e-01,  2.4268e-01,  3.7204e-01],\n",
       "         [-1.3684e+00, -2.8584e-01,  7.3511e-02, -1.1712e+00, -4.1563e-01,\n",
       "           6.9009e-02, -4.7607e-01, -9.3322e-01, -3.3467e-01, -1.1261e+00],\n",
       "         [-2.3373e+00,  1.1716e+00, -1.0939e+00, -8.9794e-01, -3.8446e-01,\n",
       "           1.0305e+00, -1.6778e+00, -1.1059e+00, -9.7706e-01,  3.8547e-02]],\n",
       "\n",
       "        [[-1.7725e-01, -1.0438e+00, -7.6756e-01, -1.9600e+00,  3.9021e-01,\n",
       "           3.0942e-01, -1.0908e-01, -7.0121e-01,  9.0913e-01, -9.8149e-01],\n",
       "         [ 3.5259e-01,  6.3631e-01, -6.9982e-01,  2.1061e-01, -1.7315e-01,\n",
       "           4.4702e-01,  3.2372e-01,  4.7855e-01,  4.8444e-01,  6.2208e-01],\n",
       "         [ 2.2756e-02, -1.7989e-01,  4.8002e-01,  1.8266e-01,  4.2362e-02,\n",
       "           4.8001e-02,  4.0304e-01,  4.0499e-01,  8.6966e-03,  6.2554e-01],\n",
       "         [-2.8182e-01,  4.7996e-01,  1.3212e+00,  6.8070e-01,  6.5138e-01,\n",
       "          -8.6290e-01, -3.1536e-01, -3.6314e-01, -9.1632e-01,  1.0776e+00],\n",
       "         [ 4.5125e-01,  9.8238e-01,  8.2040e-01,  1.0997e+00, -1.0321e+00,\n",
       "           4.0732e-01,  2.6929e-01, -9.7776e-01, -1.0242e+00, -9.2413e-01],\n",
       "         [-1.5133e+00, -4.9960e-01,  3.7981e-01, -1.2262e+00,  1.7055e-01,\n",
       "          -4.9130e-01, -2.2688e-02,  1.3783e-01,  5.4145e-02,  2.9524e-01],\n",
       "         [-1.2494e+00,  8.8501e-01,  3.5356e-01, -3.8433e-01, -1.2921e-02,\n",
       "          -2.7980e-01, -1.0503e+00, -1.5285e+00, -1.1117e+00, -4.5625e-01],\n",
       "         [-2.2398e+00, -1.6172e-01, -1.7249e+00, -1.4217e+00, -4.6979e-01,\n",
       "           9.4916e-01, -9.0193e-01,  5.6120e-01,  5.1929e-01,  2.4082e-01]],\n",
       "\n",
       "        [[ 1.4312e-02, -1.6138e-01, -4.8507e-01, -1.1239e+00,  7.4581e-01,\n",
       "           1.9278e-02, -6.7649e-01, -9.7365e-01, -4.1312e-01, -8.2482e-01],\n",
       "         [-2.4753e-01, -3.7588e-01, -8.4200e-01, -4.1622e-01, -1.5473e+00,\n",
       "           1.3390e+00,  1.6353e+00,  1.5740e+00,  1.8193e+00,  3.8954e-01],\n",
       "         [ 4.5118e-01,  7.8302e-01,  1.4145e+00,  8.6401e-01,  1.6315e+00,\n",
       "          -1.6347e+00, -4.3695e-01, -4.9845e-01, -6.8883e-01,  1.8280e+00],\n",
       "         [ 7.2468e-01,  2.9833e-01,  1.3844e+00,  1.4432e+00, -4.0886e-01,\n",
       "          -4.0910e-01,  4.3623e-01, -1.6948e-01, -9.2582e-01, -3.0951e-01],\n",
       "         [-1.5408e+00,  4.1501e-01, -4.1865e-02, -8.5841e-01, -8.1571e-01,\n",
       "           1.2667e+00, -7.3014e-01, -1.4165e+00, -1.2128e+00, -1.0009e+00],\n",
       "         [-1.3943e+00,  6.7125e-01,  6.5987e-01, -4.3935e-01,  5.7326e-01,\n",
       "          -8.4011e-01, -5.9697e-01, -4.5745e-01, -7.2284e-01,  9.6511e-01],\n",
       "         [-1.1518e+00, -4.4834e-01, -2.7745e-01, -9.0810e-01, -9.8259e-02,\n",
       "          -3.6113e-01, -2.7452e-01,  1.3857e-01,  3.8469e-01, -2.5397e-01],\n",
       "         [-1.8992e+00, -2.1313e-01, -1.5094e+00, -1.1706e+00, -1.5325e+00,\n",
       "           1.7488e+00, -1.5648e-02,  6.3792e-01,  6.9550e-01, -3.6988e-01]],\n",
       "\n",
       "        [[-5.8581e-01, -1.1736e+00, -6.2726e-01, -1.7507e+00, -6.2838e-01,\n",
       "           9.1129e-01,  6.3505e-01,  1.2181e-01,  9.2178e-01, -1.0574e+00],\n",
       "         [ 1.8089e-01,  5.8703e-01,  9.2507e-02,  2.6513e-01,  4.1807e-02,\n",
       "          -3.4368e-01,  7.9529e-01,  6.7056e-01,  1.1218e+00,  1.5920e+00],\n",
       "         [ 1.4577e+00,  6.0139e-01,  1.4777e+00,  1.6265e+00,  5.7127e-01,\n",
       "          -1.1809e+00,  3.1464e-01, -3.0479e-01, -6.9834e-01,  4.4090e-01],\n",
       "         [-1.2674e+00, -2.6903e-01,  5.2211e-01, -5.1484e-01, -1.9244e-01,\n",
       "           4.5026e-01, -5.6320e-01, -6.0822e-01, -1.1144e+00, -3.8630e-01],\n",
       "         [-1.4218e+00,  1.5859e+00,  2.3819e-01, -7.1542e-02, -4.1300e-01,\n",
       "           9.1786e-01, -1.3044e+00, -2.0118e+00, -1.9898e+00, -3.3106e-01],\n",
       "         [-1.2967e+00, -6.6210e-01,  2.8854e-02, -9.6312e-01,  4.8792e-01,\n",
       "          -9.2144e-01,  1.7885e-01,  1.2096e+00,  7.7351e-01,  1.1674e+00],\n",
       "         [-8.1126e-01, -4.9976e-01, -6.1937e-02, -6.5702e-01, -1.1610e+00,\n",
       "           4.3855e-01,  6.1176e-01,  2.1529e-01,  5.6090e-01, -8.6467e-01],\n",
       "         [-9.5542e-01, -2.7399e-02, -1.0345e+00, -3.8706e-01, -1.2808e+00,\n",
       "           1.2219e+00,  5.9042e-02,  3.8028e-01,  5.6958e-01, -4.6156e-01]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q(x_embeddings_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5a1c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the existing vector through a trainable linear transformation\n",
    "\n",
    "Q, K, V = W_q(x_embeddings_total), W_k(x_embeddings_total), W_v(x_embeddings_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791be25d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
